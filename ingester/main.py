import os
import requests
from pymilvus import AnnSearchRequest, DataType, Function, FunctionType, MilvusClient, model, RRFRanker
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

EMBEDDING_FN = model.DefaultEmbeddingFunction()
CHUNK_SIZE = 1000
VECTOR_DIM = EMBEDDING_FN.dim
COLLECTION_NAME = 'pdf_collection'
DATASET_DIR = './datasets'
CLIENT = MilvusClient(uri=os.getenv('MILVUS_ADDR'))
TEXT_SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_SIZE/5, # Overlap to maintain context between chunks
    length_function=len,
    is_separator_regex=False,
)

def define_collection() -> None:
    schema = CLIENT.create_schema(auto_id=True)
    bm25_function = Function(
        name="text_bm25_emb",
        input_field_names=["text"],
        output_field_names=["vector_sparse"],
        function_type=FunctionType.BM25, # currently the only function type for sparse in Milvus
    )
    schema.add_function(bm25_function) # to allow full-text-search
    schema.add_field(
        field_name='id',
        datatype=DataType.INT64,
        is_primary=True,
        description='document id',
        auto_id=True,
    )
    schema.add_field(
        field_name='text',
        datatype=DataType.VARCHAR,
        max_length=int(CHUNK_SIZE*1.1), # give 110%
        enable_analyzer=True, # enable full-text search
        description='document chunked text for full-text search',
    )
    schema.add_field(
        field_name='vector_dense',
        datatype=DataType.FLOAT_VECTOR,
        dim=VECTOR_DIM,
        enable_analyzer=True,
        description='document chunked text dense vector',
    )
    schema.add_field(
        field_name='vector_sparse',
        datatype=DataType.SPARSE_FLOAT_VECTOR,
        description='document chunked text sparse embedding auto-generated by the built-in BM25 Function',
    )
    schema.add_field(
        field_name='metadata',
        datatype=DataType.JSON,
        description='document metadata',
    )
    index_params = CLIENT.prepare_index_params()
    index_params.add_index(
        field_name='vector_dense',
        index_name='vector_dense_index',
        index_type='AUTOINDEX', # Need to compare with IVF_FLAT
        metric_type='COSINE', # Need to compare with L2 (Euclidean)
    )
    index_params.add_index(
        field_name='vector_sparse',
        index_name='vector_sparse_index',
        index_type='SPARSE_INVERTED_INDEX',
        metric_type='BM25',
        params={"inverted_index_algo": "DAAT_MAXSCORE"}, # need to compare another algo
    )

    if CLIENT.has_collection(COLLECTION_NAME):
        CLIENT.drop_collection(COLLECTION_NAME)

    CLIENT.create_collection(
        collection_name=COLLECTION_NAME,
        schema=schema,
        index_params=index_params,
    )

def load_datasets() -> None:
    for root, _, files in os.walk(DATASET_DIR):
        for file in files:
            if file.lower().endswith('.pdf'):
                file_path = os.path.join(root, file)
                docs = PyPDFLoader(file_path).load()
                chunks = TEXT_SPLITTER.split_documents(docs)
                data = []
                for i in range(len(chunks)):
                    chunk = chunks[i]
                    vector = vectorize([chunk.page_content])
                    data.append({
                        "vector_dense": vector[0],
                        "text": chunk.page_content[:CHUNK_SIZE],
                        "metadata": chunk.metadata,
                    })
                CLIENT.insert(
                    collection_name=COLLECTION_NAME,
                    data=data,
                )

def search(query: str, limit: int = 2, output_fields: list = ['text', 'metadata']) -> list:
    vector_search = {
        "data": vectorize([query]),
        "anns_field": "vector_dense",
        "param": {"nprobe": 10},
        "limit": limit,
    }
    fulltext_search = {
        "data": [query],
        "anns_field": "vector_sparse",
        "param": {"drop_ratio_search": 0.2},
        "limit": limit
    }
    reqs = [
        AnnSearchRequest(**vector_search),
        AnnSearchRequest(**fulltext_search),
    ]
    
    res = CLIENT.hybrid_search(
        collection_name=COLLECTION_NAME,
        reqs=reqs,
        ranker=RRFRanker(),
        limit=limit,
        output_fields=output_fields,
    )
    return [
        hit
        for hits in res
        for hit in hits
    ]

def vectorize(texts: list[str]) -> list[list[float]]:
    url = os.getenv('MODEL_GARDEN_URL') or ''
    if url == '':
        return EMBEDDING_FN.encode_documents(texts)

    headers = {
            'Content-Type': 'application/json',
    }
    payload = {
            "model": os.getenv('MODEL_NAME'),
            "input": texts,
            "encoding_format": "float",
    }
    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    result = response.json()
    embeddings = [item['embedding'] for item in result['data']]
    return embeddings

def main():
    define_collection()
    load_datasets()
    for hit in search('why do we need barito?', limit=3):
        print(f'id: {hit.id}\ndistance: {hit.distance}\ntext: {hit.entity.text}\nmetadata: {hit.metadata}\n')



if __name__ == "__main__":
    main()
